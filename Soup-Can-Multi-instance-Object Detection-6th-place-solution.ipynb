{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104781,"databundleVersionId":12610790,"sourceType":"competition"},{"sourceId":12414655,"sourceType":"datasetVersion","datasetId":7779550}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ¥« 6th Place Solution: Soup Can Multi-instance Object Detection (0.989 mAP)\n\n## ðŸ§  Approach Overview\n\nThis solution uses **YOLOv11** to detect and locate Soup Cans in images. By carefully adjusting the training process and enriching the dataset with synthetic images generated by **Falcon**, we achieved a **0.989 mAP** score in the Multi-instance Object Detection Challenge.\n\n---\n\n## ðŸ“ 1. Data Strategy\n\n- **Dataset Structure**:\n  - Used the full dataset (training + validation) provided by the competition.\n  - Added 5 synthetic data folders: `new_data`, `new_data2`, `new_data3`, `new_data4`, `new_data5`.\n  - Each folder contains 100 images (81 train, 19 val).\n  - Manually excluded images that do **not contain Soup Cans** or have **unclear bounding boxes**.\n\n- **Dataset Summary**:\n  - **Total Images**: 544\n  - **Training**: 466 images\n  - **Validation**: 78 images\n\n- **Test Set**:\n\n  - Used the **test data provided** in the competition.\n\n- **Dataset Set Link**: [Dataset](https://www.kaggle.com/datasets/younesbenalia/multi-instance-object-detection-challenge)\n\n- **Class Handling**:\n  - **Single-class detection** (Soup Cans).\n  - YOLO-style annotations.\n\n- **Augmentations**:\n  - **Mosaic** (disabled in the last 20 epochs).\n  - **HSV color space manipulations**:\n    - Hue Â±0.015\n    - Saturation Â±0.7\n    - Value Â±0.4\n  - **Spatial transforms**:\n    - Flip (50%)\n    - Translation (10%)\n    - Scaling (50%)\n  - **Image resizing** to `1056 Ã— 1056`.\n\n---\n\n## ðŸ—ï¸ 2. Model Architecture\n\n- **Model**: `YOLOv11x`\n\n- **Optimization**:\n  - Optimizer: **SGD** with momentum (0.937)\n  - Scheduler: **Cosine Learning Rate Scheduler**\n    - Initial learning rate: `0.001`\n  - **Weight Decay**: `0.0005` for regularization\n  - **Training Duration**: 300 epochs (without early stopping)\n  - **Warmup Settings**:\n    - `warmup_epochs = 3`\n    - `warmup_momentum = 1`\n\n- **Metric Target**:\n  - Optimized for **mAP@0.5 IoU**\n\n---\n\n## ðŸ”Ž 3. Inference Enhancements\n\n- **Model Selection**:\n  - Used the checkpoint that performed best on the validation set: `best.pt`\n\n- **Test-Time Augmentation (TTA)**:\n  - Flipping and minor zooming during inference to improve predictions.\n\n- **Detection Parameters**:\n  - **IoU Threshold**: `0.4`\n  - **Max Detections**: `600` per image\n  - **Confidence Threshold**: `0` (favoring high recall)\n\n- **Multiscale Inference**:\n  - Images were evaluated at 5 different scales:\n    - `640`, `1056`, `1440`, `1920`, `2560`\n  - Predictions were captured separately for each size.\n\n- **Post-Processing**:\n  - **Invalid box filtering**\n  - **Weighted Box Fusion (WBF)**:\n    - IoU threshold: `0.5`\n    - Skip box threshold: `0.01`\n\n---\n\n## ðŸ“¤ 4. Submission\n\n- **Prediction Validation**:\n  - Strict format checking to comply with submission requirements.\n  - Automatic handling for images with **no detections**.\n\n- **Submission Format**:\n  - Final predictions are converted into the required format.\n  - **Output File**: `submission_wbf_0.5.csv`\n\n---\n\n## â™»ï¸ Reproducibility\n\n- Fixed random seeds across the entire training pipeline.\n- **Hardware** used: `NVIDIA Tesla T4 GPU`.\n\n---\n\n## ðŸ“ Additional Notes\n\n- Synthetic data proved critical for improving generalization due to limited original training images.\n- Using a **zero confidence threshold** paired with **WBF** allowed for aggressive box fusion, significantly boosting mAP.\n- The pipeline was designed to be **robust to overfitting** through heavy augmentations and proper validation splits.\n\n---\n\n## ðŸ“‡ Contact Information\n\nIf you'd like to connect, collaborate, or ask questions, feel free to reach out:\n\n- **ðŸ‘¨â€ðŸ’» Author**: Younes Benalia  \n- **ðŸ“§ Email**: [younes.benalia.dz@gmail.com]  \n- **ðŸ”— LinkedIn**: [younesbenalia](https://www.linkedin.com/in/younesbenalia)  \n- **ðŸ“Š Kaggle**: [younesbenalia](https://www.kaggle.com/younesbenalia)  \n- **ðŸ™ GitHub**: [younesBenalia](https://github.com/younesBenalia)\n- **ðŸ™ Huggingface**: [younesbenalia](https://huggingface.co/younesbenalia)  \n- **ðŸ“Š Kaggle notebook**: [notebook](https://www.kaggle.com/code/younesbenalia/soup-can-detection-6th-place-solution)  \n---\n\n","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install ultralytics\n!pip install ensemble-boxes\nclear_output()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nfrom pathlib import Path\nimport csv\nimport os\nimport random\nimport torch\n# Set random seeds for reproducibility\nnp.random.seed(42)\nrandom.seed(42)\ntorch.manual_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset filtering","metadata":{}},{"cell_type":"code","source":"# === Dataset base path ===\nbase_path = Path(\"/kaggle/input/d/younesbenalia/multi-instance-object-detection-challenge/Starter_Dataset\")\n\n# === Image folders for train and val (from your YAML)\ntrain_folders = [\n    \"clutter/train/images\",\n    \"couch_far_10/train/images\",\n    \"far_10_half_clutter/train/images\",\n    \"film_grain_10_half_clutter/train/images\",\n    \"large_plant_10/train/images\",\n    \"new_data/train/images\",\n    \"new_data2/train/images\",\n    \"new_data3/train/images\",\n    \"new_data4/train/images\",\n    \"new_data5/train/images\",\n    \"no_clutter_10/train/images\",\n    \"table_close_10/train/images\"\n]\n\nval_folders = [\n    \"clutter/val/images\",\n    \"couch_far_10/val/images\",\n    \"far_10_half_clutter/val/images\",\n    \"film_grain_10_half_clutter/val/images\",\n    \"large_plant_10/val/images\",\n    \"new_data/val/images\",\n    \"new_data2/val/images\",\n    \"new_data3/val/images\",\n    \"new_data4/val/images\",\n    \"new_data5/val/images\",\n    \"no_clutter_10/val/images\",\n    \"table_close_10/val/images\"\n]\n\n# === Images to exclude (full paths)\nexcluded_images = {\n    str(base_path / \"couch_far_10/train/images/000000004.png\"),\n    str(base_path / \"far_10_half_clutter/train/images/000000004.png\"),\n    str(base_path / \"new_data/train/images/000000018.png\"),\n    str(base_path / \"new_data/train/images/000000049.png\"),\n    str(base_path / \"new_data/train/images/000000051.png\"),\n    str(base_path / \"new_data/train/images/000000069.png\"),\n    str(base_path / \"new_data/train/images/000000073.png\"),\n    # str(base_path / \"new_data/train/images/000000076.png\"),\n    str(base_path / \"new_data/val/images/000000003.png\"),\n    str(base_path / \"new_data2/train/images/000000012.png\"),\n    str(base_path / \"new_data2/train/images/000000029.png\"),\n    str(base_path / \"new_data2/train/images/000000030.png\"),\n    str(base_path / \"new_data2/train/images/000000039.png\"),\n    str(base_path / \"new_data2/train/images/000000062.png\"),\n    str(base_path / \"new_data2/train/images/000000067.png\"),\n    str(base_path / \"new_data2/train/images/000000079.png\"),\n    str(base_path / \"new_data3/train/images/000000013.png\"),\n    str(base_path / \"new_data3/train/images/000000036.png\"),\n    str(base_path / \"new_data3/train/images/000000055.png\"),\n    str(base_path / \"new_data3/train/images/000000059.png\"),\n    str(base_path / \"new_data5/train/images/000000018.png\"),\n    str(base_path / \"new_data2/val/images/000000001.png\"),\n    str(base_path / \"new_data2/val/images/000000087.png\"),\n    str(base_path / \"new_data2/val/images/000000090.png\"),\n    str(base_path / \"new_data3/val/images/000000002.png\"),\n    str(base_path / \"new_data4/val/images/000000008.png\"),\n    str(base_path / \"new_data5/val/images/000000001.png\"),\n  \n}\n \n\ndef build_image_list(folders, split_name, output_txt_path):\n    image_paths = []\n\n    for folder in folders:\n        full_dir = base_path / folder\n        for img_path in full_dir.glob(\"*\"):\n            if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n                if str(img_path) not in excluded_images:\n                    image_paths.append(str(img_path))\n\n    # Save to txt\n    output_path = Path(\"/kaggle/working\") / output_txt_path\n    with open(output_path, \"w\") as f:\n        f.write(\"\\n\".join(image_paths))\n\n    print(f\"[âœ…] Saved {len(image_paths)} image paths to {output_path}\")\n\nbuild_image_list(train_folders, \"train\", \"train_filtered.txt\")\nbuild_image_list(val_folders, \"val\", \"val_filtered.txt\")\n\n\ndata_yaml = \"\"\"\ntrain: /kaggle/working/train_filtered.txt\nval: /kaggle/working/val_filtered.txt  # Optional - same process for val\n\nnc: 1\nnames: ['Soup']\n\"\"\"\nwith open('custom_data.yaml', 'w') as file:\n    file.write(data_yaml)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Trainng","metadata":{}},{"cell_type":"code","source":"model = YOLO(\"yolo11x.pt\")\n# data_yaml = \"/kaggle/input/d/younesbenalia/multi-instance-object-detection-challenge/Starter_Dataset/yolo_params.yaml\"\ndata_yaml = \"/kaggle/working/custom_data.yaml\"\n\n\nmodel.train(\n    data=data_yaml,\n    epochs=300,                \n    batch=4,                   \n    imgsz=1056,\n    patience=300,               \n    optimizer='SGD',\n    momentum=0.937,          \n    lr0=0.001,                \n    weight_decay=0.0005,       \n    cos_lr=True,               \n    save_period=5,             \n    workers=8,\n    # Augmentations\n    close_mosaic=20,\n    hsv_h=0.015,\n    hsv_s=0.7,\n    hsv_v=0.4,\n    flipud=0,\n    fliplr=0.5,\n    translate=0.1,\n    scale=0.5,\n    shear=0,\n    warmup_epochs= 3,\n    warmup_momentum= 1,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference & Post-Processing","metadata":{}},{"cell_type":"code","source":"def filter_invalid_boxes(boxes, scores, labels):\n    filtered_boxes, filtered_scores, filtered_labels = [], [], []\n    for b, s, l in zip(boxes, scores, labels):\n        if abs(b[2] - b[0]) > 1e-6 and abs(b[3] - b[1]) > 1e-6:\n            filtered_boxes.append(b)\n            filtered_scores.append(s)\n            filtered_labels.append(l)\n    return filtered_boxes, filtered_scores, filtered_labels\n    \ndef run_inference(models, image_sizes, test_images_path):\n    image_paths = [p for p in Path(test_images_path).glob(\"*\") if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n    predictions = {}\n\n    for model_idx, model in enumerate(models):\n        model.eval()\n        model.training = False\n        predictions[model_idx] = {}\n        for size in image_sizes:\n            predictions[model_idx][size] = {}\n            pred = []\n            for img_path in image_paths:\n                image_id = img_path.stem\n                image = Image.open(img_path)\n                img_width, img_height = image.size\n\n                results = model.predict(source=str(img_path), conf=conf,iou=0.4, max_det=600, augment=True, imgsz=size, verbose=False)\n                boxes, scores, labels = [], [], []\n\n                for result in results:\n                    if result.boxes is None:\n                        continue\n                    boxes = result.boxes.xyxy.cpu().numpy().tolist()\n                    scores = result.boxes.conf.cpu().numpy().tolist()\n                    labels = result.boxes.cls.cpu().numpy().tolist()\n\n                    norm_boxes = [\n                        [x1 / img_width, y1 / img_height, x2 / img_width, y2 / img_height]\n                        for x1, y1, x2, y2 in boxes\n                    ]\n                    norm_boxes, scores, labels = filter_invalid_boxes(norm_boxes, scores, labels)\n\n                predictions[model_idx][size][image_id] = {\n                    \"boxes\": norm_boxes,\n                    \"scores\": scores,\n                    \"labels\": labels\n                }\n                \n                if boxes:\n                    prediction_string = \" \".join(\n                        f\"{int(lbl)} {score:.6f} {(b[0]+b[2])/2:.6f} {(b[1]+b[3])/2:.6f} {(b[2]-b[0]):.6f} {(b[3]-b[1]):.6f}\"\n                        for b, score, lbl in zip(norm_boxes, scores, labels)\n                    )\n                else:\n                    prediction_string = \"no boxes\"\n\n                pred.append({\n                    \"image_id\": image_id,\n                    \"prediction_string\": prediction_string\n                })\n\n            # Save CSV per model and size\n            df = pd.DataFrame(pred)\n            csv_path = f\"submission_{model_idx}_{size}.csv\"\n            df.to_csv(csv_path, index=False, quoting=csv.QUOTE_MINIMAL)\n            print(f\"[saved] {csv_path}\")\n            print(df.head(10))\n\n    return predictions\n\ndef apply_wbf_and_save_final_submission(predictions, image_ids, output_path=\"submission_wbf.csv\"):\n    wbf_results = []\n\n    for image_id in image_ids:\n        all_boxes, all_scores, all_labels = [], [], []\n\n        for model_preds in predictions.values():\n            for size_preds in model_preds.values():\n                if image_id not in size_preds:\n                    continue\n                pred = size_preds[image_id]\n                if not pred[\"boxes\"]:\n                    continue\n                all_boxes.append(pred[\"boxes\"])\n                all_scores.append(pred[\"scores\"])\n                all_labels.append(pred[\"labels\"])\n\n        if not all_boxes:\n            pred_str = \"no boxes\"\n        else:\n            fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n                all_boxes, all_scores, all_labels, iou_thr=iou_thr, skip_box_thr=skip_box_thr\n            )\n\n            pred_str = \" \".join(\n                f\"{int(lbl)} {score:.6f} {(b[0]+b[2])/2:.6f} {(b[1]+b[3])/2:.6f} {(b[2]-b[0]):.6f} {(b[3]-b[1]):.6f}\"\n                for b, score, lbl in zip(fused_boxes, fused_scores, fused_labels)\n            )\n\n        wbf_results.append({\n            \"image_id\": image_id,\n            \"prediction_string\": pred_str\n        })\n\n    wbf_df = pd.DataFrame(wbf_results)\n    wbf_df.to_csv(output_path, index=False, quoting=csv.QUOTE_MINIMAL)\n    print(f\"[notice] âœ… WBF submission saved to {output_path}\")\n    print(wbf_df.head(10))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport pandas as pd\nimport csv\nfrom ultralytics import YOLO\nfrom ensemble_boxes import weighted_boxes_fusion\nfrom PIL import Image\n\nmodel_paths = [\n    \"/kaggle/working/runs/detect/train/weights/best.pt\",\n]\n\ntest_images_path = \"/kaggle/input/multi-instance-object-detection-challenge/Starter_Dataset/TestImages/images\"\noutput_dir = \"/kaggle/working/predictions/labels\"\nconf = 0\niou_thr = 0.5\nskip_box_thr = 0.01\nimage_sizes = [640, 1056, 1440, 1920, 2560]\n\nmodels = [YOLO(path) for path in model_paths]\npredictions = run_inference(models, image_sizes, test_images_path)\n\nimage_ids = list(next(iter(next(iter(predictions.values())).values())).keys())\n\napply_wbf_and_save_final_submission(predictions, image_ids, output_path=f\"submission_wbf_{iou_thr}.csv\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}